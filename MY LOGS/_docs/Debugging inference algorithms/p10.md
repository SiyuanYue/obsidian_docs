# Debugging inference algorithms

##  优化验证测试 (ch44)

假设你正在构建一个语音识别系统，该系统通过输入一个音频片段 $A​$ ，并为每一个可能的输出句子 $S​$ 计算得分 $\text{Score}_A(S)​$。例如，你可以试着估计 $\text{Score}_A(S)=P(S\|A)​$ ，表示句子 $S​$ 是正确输出的转录的概率，其中 $A​$ 是给定的输入音频。

给定某种方法能够计算 $\text {Score}_A(S)$ 后，你仍然需要找到一个英文句子 $S$ 来使之最大化：

$$
\text{Output} = \arg \max_s \text{Score}_A(S)
$$

要如何去计算上面的 $\arg \max$ 呢？假设在英文中共有 5000 个词汇，对于长度为 $N$ 的句子，则有 $50000^N$ 种搭配，多到根本无法一一列举。因此，你需要使用一种近似搜索算法，努力去找到能够优化（最大化）$\text{Score}_A(S)$ 的那个 $S$ . 有一种叫做 “定向搜索” 的搜索算法，在搜索过程中仅保留最优的 $K$ 个候选项（在本章中你并不需要了解该算法的细节）。类似这样的算法并不足以保证能够找到满足条件的 $S$ 来最大化 $\text{Score}_A(S)$ .

假设有一个音频片段记录着某人说的：“我爱机器学习。”但你的系统输出的却是不正确的 “我爱机器人。”，它没能够输出正确的转录。造成该误差的可能原因有两种：

1. **搜索算法存在问题。** 近似搜索算法没能够找到最大化 $\text{Score}_A(S)$ 的那个 $S$ .
2. **目标（得分函数）存在问题。** 我们对  $\text{Score}_A (S)=P(S\|A)$ 的估计并不准确。尤其在此例中，我们的得分函数没能辨认出 “我爱机器学习” 是正确的转录。

对应不同的失败原因，你需要优先考虑的工作方向也将很不一样。如果原因 1 导致了问题，你应该改进搜索算法。如果原因 2 导致了问题，你应该在评估学习算法的函数  $\text{Score}_A(S)$  上面多花些心思。

面对这种情况，一些研究人员将决定研究搜索算法；其他人则努力去找到更好的  $\text{Score}_A(S)$ . 但是，除非你知道其中哪一个是造成误差的潜在原因，否则你的努力可能会被浪费掉。怎么样才能更系统地决定要做什么呢？

让我们用 $S_{out}$ 表示实际的输出 “我爱机器人”，用 $S^\*$ 表示正确的输出 “我爱机器学习” 。为了搞清楚上面的 1 或 2 是否存在问题，你可以执行 **优化验证测试（Optimization Verification test）**： 首先计算  $S_{out}$ 和  $S^\*$ ，接着比较他们的大小。有两种可能：

情况1：$\text{Score}\_A {(S^\*)} \gt \text{Score}\_A {(S_{out})}$

在这种情况下，你的学习算法正确地给了 $S^\*$ 一个比  $S_{out}$ 更高的分数。尽管如此，我们的近似搜索算法选择了  $S_{out}$ 而不是 $S^\*$ . 则表示你的近似搜索算法没能够找到最大化 $\text{Score}_A(S)$ 的那个 $S$ . 此时优化验证测试告诉你，搜索算法存在着问题，应该花时间研究。例如，你可以尝试增加定向搜索的搜索宽度。

 情况2：$\text{Score}\_A (S^\*) \leq \text{Score}\_A (S_{out})$

在这种情况下，计算 $\text{Score}\_A (.)$ 的方式是错误的：它没有给正确的输出 $S^\*$ 比实际输出  $S_{out}$ 一个相同或更高的分数。优化验证测试告诉你，目标（得分函数）存在问题。因此，你应该专注于改进你的算法对不同的句子 $S$ 学习或近似出得分 $\text{Score}_A (S)$ 的方式。

我们上面的讨论集中于某个单一的样本 $A$ 上，想要在实践中运用优化验证测试，你需要在开发集中检测这些误差样本。对于每一个误差样本，你都需要测试是否有 $\text{Score}\_A (S^\*) \gt  \text{Score}\_A (S)\_{out}$ . 开发集中所有满足该不等式的样本都将被标记为优化算法自身所造成的误差，而满足不等式 $\text{Score}\_A (S^\*) \leq  \text{Score}\_A (S)_{out}$ 的样本将被记为是计算得分 $\text{Score}_A (.)$  造成的误差。

假设你最终发现 95% 的误差是得分函数 $\text{Score}_A (.)$  造成的，而仅有 5% 的误差是由优化算法造成的。现在你应该知道了，无论你如何改进你的优化程序，实际上也只会消除误差中的 5% 左右。因此，你应该专注于改进你的得分函数  $\text{Score}_A (.)$  .

## 优化验证测试的一般形式   (ch45)


你可以在如下情况运用优化验证测试，给定输入 $x$ ，且知道如何计算  $\text{Score}_x (y)$  来表示 $y$ 对输入 $x$ 的响应好坏。此外，你正在使用一种近似算法来尽可能地找到 $\arg \max_y \text{Score}_x(y)$ ，但却怀疑该搜索算法有时候并不能找到最大值。在我们先前提到的语音识别的例子中，$x=A$ 代表某个音频片段， $y=S$ 代表输出的转录。

假设 $y^\*​$ 是 “正确的” 输出，可算法输出了 $y_{out}​$ . 此时的关键在于测量是否有 $\text{Score}\_x(y^\*) \ge \text{Score}\_x(y\_{out})​$ . 如果该不等式成立，我们便可以将误差归咎于优化算法。（请参考前一章的内容，以确保你理解这背后的逻辑。）否则，我们将误差归咎于  $\text{Score}_x (y)​$  的计算方式。

让我们再看一个例子：假设你正在构建一个中译英的机器翻译系统，输入一个中文句子 $C$ ，并计算出每一个可能的翻译句子 $E$ 的得分 $\text{Score}_C(E)$，例如，你可以使用 $\text{Score}_C(E)=P(E\|C)$，表示给定输入句子 $C$ ，对应翻译句子为 $E$ 的概率。

你的算法将通过计算下面的公式来进行句子的翻译：
$$
\text{Output}=\arg \max_E \text{Score}_C(E)
$$
然而所有可能的英语句子构成的集合太大了，所以你将依赖于启发式搜索算法。 

假设你的算法输出了一个错误的翻译 $E_{out}​$，而不是正确的翻译 $E^\*​$ . 优化验证测试会要求你计算是否有 $\text{Score}\_C (E^*) \gt  \text{Score}\_C (E)_{out}​$ . 如果这个不等式成立，表明 $\text{Score}_C(.)​$ 正确地辨认  $E^\*​$ 是一个更好的输出；因此你可以将把这个误差归咎于近似搜索算法。否则，你可以将这个误差归咎于 $\text{Score}_C(.)​$ 的计算方式。

在人工智能领域，这是一种非常常见的 “设计模式”，首先要学习一个近似的得分函数 $\text{Score}_X(.)$ ，然后使用近似最大化算法。如果你能够发现这种模式，就能够使用优化验证测试来理解造成误差的来源。  

## 强化学习举例  (ch46)


假设你正在用机器学习来教直升机复杂的飞行动作。下面是一张延时照片，照片上是一台电脑控制器的直升机正在引擎关闭的情况下执行着陆。

![](ch46.jpg)

这被称为“自旋”策略，即使引擎意外故障了，它也允许直升机着陆。这也是人类飞行员经常进行的训练。而你的目标是使用一种学习算法，让直升机通过一个轨迹 $T$ 安全地着陆。 

要应用强化学习策略，你必须设计一个 “奖励函数” $R(.)$，它给出一个分数来衡量每一个可能轨迹 $T$ 的好坏。例如，如果 $T$ 导致直升机坠毁，那么奖励也许是 $R(T)=-1000$ ，这是一个巨大的负反馈；而一个导致安全着陆的轨迹 $T$ 可能会产生一个正的 $R(T)$ 值，它的精确值取决于着陆过程的平稳程度。奖励函数 $R(.)$ 通常是人为选择的，以量化不同轨迹 $T$ 的理想程度。它必须权衡考虑着陆的颠簸程度，直升机是否降落在理想的位置，乘客的降落体验等因素。设计一个好的奖励函数并非易事。

给定一个奖励函数 $R(T)$ ，强化学习算法的工作是控制直升机，使其达到 $\max_TR(T)$ . 然而，强化学习算法原理内部有许多近似操作，可能无法成功实现这种最大化需求。

假设你已经选择了某些奖励函数 $R(T)$ 作为反馈，并运行了学习算法。然而它的表现似乎比人类飞行员要糟糕得多——它更加颠簸，而且似乎不那么安全。你如何判断错误是否由强化学习算法造成——它试图找到一个轨迹 $T$ ，满足 $\max_TR(T)$ ——或者错误来自于你的奖励函数——它尝试衡量并且指定一种在颠簸程度和着陆精度之间权衡的理想结果。

为了应用优化验证测试，让 $T_{human}$ 表示人类飞行员所选择的轨迹，并让 $T_{out}$ 代表算法所选择的轨迹。根据我们上面的描述， $T_{human}$ 是优于  $T_{out}$ 的发展轨迹。因此，关键的测试点在于：不等式 $R(T_{human}) \gt R(T_{out})$ 是否成立？ 

情况 1：如果不等式成立，奖励函数  $R(.)$ 正确地使 $T_{human}$ 优于  $T_{out}$ ，但这表明我们的强化学习算法找到的  $T_{out}$ 仍不够好，花时间去改进算法是很值得的。

情况 2：如果上面不等式不成立，而是 $R(T_{human}) \leq R(T_{out})$ . 这表明  $R(.)$ 的设计使得理应是更优策略的  $T_{human}$ 得到了一个更糟的评分。你应当致力于改进  $R(.)$ ，以更好地获得与良好着陆情况相对应的权衡。 

许多机器学习应用程序使用这种优化某个近似的 “模式” 来确定得分函数 $\text{Score}_X(.)$ . 有时没有特定的输入 $x$ ，形式简化为 $\text{Score}X(.)$  。在上面的例子中，得分函数即是奖励函数，$\text{Score(T)=R(T)}$，而采用的优化算法是强化学习算法，目的是找到好的轨迹 $T$ .

这和前面的例子有一个区别，那就是，与其比较 “最优” 输出，不如将其与人类水平的表现进行比较。我们认为，即使  $T_{human}$ 不是最优的，它也是相当不错的。一般而言，只要有一个比当前学习算法性能更好的输出  $y^\*$ （在这个例子中即是指  $T_{human}$ ），即使它不是 “最优” 的，优化验证测试也能够反映改进学习算法与改进得分函数之间哪一个更具前途。


