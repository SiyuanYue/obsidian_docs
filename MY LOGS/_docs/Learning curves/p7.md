#  Learning curves
## 诊断偏差与方差：学习曲线     (ch28)

我们现在已经知道了一些方法，来估计可避免多少由偏差与方差所导致的误差——人们通常会估计最优错误率，以及算法在训练集和测试集上的误差。下面让我们来讨论一项更有帮助的技术：绘制学习曲线。

学习曲线可以将开发集的误差与训练集样本的数量进行关联比较。想要绘制出它，你需要设置不同大小的训练集运行算法。假设有 1000 个样本，你可以选择在规模为 100、200、300 ... 1000 的样本集中分别运行算法，接着便能得到开发集误差随训练集大小变化的曲线。下图是一个例子：

![](ch28_01.jpg) 

随着训练集大小的增加，开发集误差应该降低。 

我们通常会设置一些“期望错误率”，并希望学习算法最终能够达到该值。例如：

- 如果希望算法能达到人类水平的表现，那么人类错误率可能就是“期望错误率”。
- 如果学习算法为某些产品提供服务（如提供猫咪图片），我们或许将主观感受到需什么样的水平才能给用户提供出色的体验。
- 如果你已经从事一项应用很长时间，那么你可能会有一种直觉，预判在下一个季度里你会有多大的进步。 

将期望的表现水平添加到你的学习曲线中： 

![](ch28_02.jpg)

显而易见，你可以根据红色的“开发误差”曲线的走势来推测，在添加一定量的数据后，曲线距离期望的性能接近了多少。在上面的例子中，将训练集的大小增加一倍可能会让你达到期望的性能，这看起来是合理的。 

![](ch28_03.jpg)

因此，观察学习曲线或许能帮到你，避免了花费几个月的时间去收集两倍的训练数据，到头来却发现这并不管用的情况。 

该过程的一个缺点是，如果你只关注了开发错误曲线，当数据量变得越来越多时，将很难预测后续红色曲线的走向。因此我们会选择另外一条曲线来协助评估添加数据所带来的影响：即训练误差曲线。

## 绘制训练误差曲线  (ch29)


随着训练集大小的增加，开发集（和测试集）误差应该会降低，但你的训练集误差往往会随之增加。 

让我们来举例说明一下。假设你的训练集只有两个样本：一张猫图和一张非猫图。学习算法很轻易地就可以“记住”训练集中这两个样本，并且训练集错误率为 0%. 即使有一张或两张的样本图片被误标注了，算法也能够轻松地记住它们。

现在假设你的训练集包含 100 个样本，其中有一些样本可能被误标记，或者是模棱两可的（图像非常模糊），所以即使是人类也无法分辨图中是否有一只猫。此时，或许学习算法仍然可以“记住”大部分或全部的训练集，但很难获得 100% 的准确率。通过将训练集样本数量从 2 个增加到 100 个，你会发现训练集的准确率会略有下降。

下面我们将训练误差曲线添加到原有的学习曲线中：

![](ch29_01.jpg)

可以看到，蓝色的“训练误差”曲线随着训练集大小的增加而上升，而且算法在训练集上通常比在开发集上表现得更好；因此，红色的开发误差曲线通常严格位于蓝色训练错误曲线之上。 

让我们来讨论一下如何解释这些曲线的含义。 

## 解读学习曲线：高偏差 (ch30)


假设你的开发误差曲线如下图所示：

![](ch30_01.jpg)

我们之前提到，如果开发误差曲线趋于平稳，则不太可能通过添加数据来达到预期的性能，但也很难确切地知道红色的开发错误曲线将趋于何值。如果开发集很小，或许会更加不确定，因为曲线中可能含有一些噪音干扰。

假设我们把训练误差曲线加到上图中，可以得到如下结果： 

![](ch30_02.jpg)

现在可以绝对肯定地说，添加更多的数据并不奏效。为什么呢？记住我们的两个观察结果：

- 随着我们添加更多的训练数据，训练误差变得更糟。因此蓝色的训练误差曲线只会保持不动或上升，这表明它正远离期望的性能水平（绿色的线）。
- 红色的开发误差曲线通常要高于蓝色的训练误差曲线。因此只要训练误差高于期望性能，通过添加更多数据来让红色开发误差曲线下降到期望性能水平之下也基本没有可能。

在同一张图中检查开发误差曲线和训练误差曲线可以让我们对推测开发误差曲线的走势更有信心。

为了便于讨论，假设期望性能是我们对最优错误率的估计。那么上面的图片就是一个标准的“教科书”式的例子（具有高可避免偏差的学习曲线是什么样的）：在训练集大小的最大处（大致对应使用我们所有的训练数据），训练误差和期望性能之间有较大的间隙，这代表可避免偏差较大。此外，如果训练曲线和开发曲线之间的间隙小，则表明方差小。

之前，我们只在曲线最右端的点去衡量训练集误差和开发集误差，这与使用所有的可训练数据训练算法相对应。绘制完整的学习曲线将为我们呈现更全面的结果图片，显示算法在不同训练集大小上的表现。

## 解读学习曲线 ：其它情况 (ch31)


考虑下面的学习曲线：

![](ch31_01.jpg)

这张图表示的是高偏差？高方差？还是两者都有？

蓝色训练误差曲线相对较低，红色的开发误差曲线比蓝色训练误差高得多。因此，偏差很小，但方差很大。添加更多的训练数据可能有助于缩小开发误差和训练误差之间的差距。 

现在考虑下面的情况：

![](ch31_02.jpg)

这种情况下，训练误差很大，它比期望的性能水平要高得多，开发误差也比训练误差大得多。因此，学习算法有着明显的偏差和方差。此时你必须找到一种方法来减少算法中的偏差和方差。

## 绘制学习曲线 

假设你有一个非常小的训练集，仅有 100 个样本。那么你可以从中随机选择 10 个样本来训练你的算法，然后是 20 个，30 个，100 个，每次增加 10 个样本。然后使用 10 个数据点来绘制你的学习曲线。你可能会发现，在较小规模的训练集上，曲线看起来带有点噪声（这意味着这些值比预期的要高/低）。

当只使用 10 个随机选择的样本进行训练时，你可能会不幸碰到特别“糟糕”的训练集，比如含有很模糊的或者误标记的样本。你当然也有可能会幸运地碰到特别“棒”的训练集。训练集的规模较小意味着开发和训练误差将随机波动。

如果你的机器学习应用程序很倾向于某一个类（如猫分类任务的负面样本比例远远大于正面样本），或者说有大量的类（如识别 100 种不同的动物物种），那么选择一个“非代表性”或糟糕的特殊训练集的几率也将更大。例如，假设你的整个样本中有 80% 是负样本（y=0），只有 20% 是正样本（y=1），那么一个含有 10 个样本的训练集就有可能只包含负样本，因而算法很难从中学到有意义的东西。

存在训练集噪声致使难以正确理解曲线的变化时，有两种解决方案：

- 与其只使用 10 个样本训练单个模型，不如从你原来的 100 个样本中进行随机有放回抽样，选择几批（比如 3-10 ）不同的 10 个样本进行组合。在这些数据上训练不同的模型，并计算每个模型的训练和开发错误。最终，计算和绘制平均训练集误差和平均开发集误差。 
- 如果你的训练集偏向于一个类，或者它有许多类，那么选择一个“平衡”子集，而不是从 100 个样本中随机抽取 10 个训练样本。例如，你可以确保这些样本中的 2/10 是正样本，8/10 是负样本。更常见的做法是，确保每个类的样本比例尽可能地接近原始训练集的总体比例。 

> 此处**有放回抽样**的意思是：你会从 100 个样本中随机选择 10 个不同的样本来生成第一个训练集，在生成第二个训练集时，你需要再次选择 10 个样本，且抽样来源仍需包括第一次选择的 10 个样本在内。因此，某一个样本可能在第一个训练集和第二个训练集都有出现。相反，如果你在无放回的情况下进行抽样，那么第二个训练集将从第一次没有被选择的 90 个样本中选出。在实践中，用有放回抽样和无放回抽样的差异不大，但是前者更为常见。

除非你已经尝试过绘制学习曲线，并得出了曲线太过嘈杂且无法看到潜在趋势的结论，否则我将不会考虑使用这两种技术。因为当你的训练集规模很大——比如超过 10000 个样本——而且类分布不是很倾斜时，你可能就不需要这些技巧了。 

最后提一点，绘制一个学习曲线的成本可能非常高：例如，你可能需要训练 10 个模型，其中样本规模可以是 1000 个，然后是 2000 个，一直到 10000 个。使用小数据集训练模型比使用大型数据集要快得多。因此，你可以用 1000、2000、4000、6000 和 10000 个样本来训练模型，而不是像上面那样将训练集的大小均匀地间隔在一个线性的范围内。这仍然可以让你对学习曲线的变化趋势有一个清晰的认识。当然，这种技术只有在训练所有额外模型所需的计算成本很重要时才有意义。