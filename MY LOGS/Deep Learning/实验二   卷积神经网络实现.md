# <center>实验二 卷积神经网络实现</center>
<center>岳思源    120L021112</center>

# 1. 实验环境
`CUDA 12.1` ,  `pytorch-cuda=11.8` , `python 3.8` , `TensorBoard` , `RTX2060`
# 2. 实验内容
>基于 PyTorch 实现 AlexNet \[1]结构, 在 Caltech101 数据集上进行验证，并使用 tensorboard 进行训练数据可视化，绘制 Loss 曲线，如有条件，尝试不同参数的影响，尝试其他网络结构。

## 2.1 预处理数据集
使用 `transforms.Compose()` 打包预处理数据集，进行 `resize`, 归一化等操作：
```python
pre_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),  # 随机水平翻转
        transforms.RandomVerticalFlip(),  # 随机竖直翻转
        transforms.Resize(size=(224, 224)),  
        transforms.ToTensor(),  # 将图片变为tensor
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 归一化
    ])
```
## 2.2 导入并按照比例划分数据集
```python
dataset = ImageFolder(dataset_dir, transform=pre_transform)
    length = len(dataset)
    ratios[0] = int(length * ratios[0])
    ratios[1] = int(length * ratios[1])
    ratios[2] = length - ratios[1] - ratios[0]
    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, ratios)
```

## 2.3 构建 AlexNet 网络模型
![[Pasted image 20230507211155.png]]
如图通过 `nn. Sequential()` API 组合搭建一层层网络：
```python
from torch import nn
import torch.optim as optim
class AlexNet(torch.nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__() 
        self.net = nn.Sequential(
   
        nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),
        nn.MaxPool2d(kernel_size=3, stride=2),
        # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
        nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
        nn.MaxPool2d(kernel_size=3, stride=2),
        # 使用三个连续的卷积层和较小的卷积窗口。
        nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
        nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
        nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),
        nn.MaxPool2d(kernel_size=3, stride=2),
        nn.Flatten(),
        # 使用dropout层减轻过拟合
        nn.Linear(6400, 4096), nn.ReLU(),
        nn.Dropout(p=0.5),
        nn.Linear(4096, 4096), nn.ReLU(),
        nn.Dropout(p=0.5),
        # 最后是输出层。
        nn.Linear(4096,101)   )
    def forward(self,X):
        return self.net(X)
```

## 2.4  设置超参数和训练网络
设置了这两组超参数，训练了两次，结果在 *3* 中分析
`train_loader, valid_loader, test_loader, out_features=load_caltech_101 (32)`
`lr, num_epochs = 0.01, 20`
`log_batch = 5`

`train_loader, valid_loader, test_loader, out_features=load_caltech_101 (64)`
`lr, num_epochs = 0.005, 40`
`log_batch = 5`
使用 CUDA, 选择优化器和损失函数, 使用` tensorboard `记录
```python
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()

    if not os.path.exists('./log'):
        os.makedirs('./log')
    writer = SummaryWriter(log_dir='./log', comment='AlexNet')  # 初始化SummaryWriter，用于记录要可视化的数据
```

迭代训练模型：
```python
for epoch in range(num_epochs):
        train_loss = .0
        x_train_num = len(train_loader.dataset)
        batch_num = len(train_loader)

        net.train()
        for i, (X, y) in enumerate(train_loader):
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            if i % int((1 / log_batch) * batch_num) == 0 and verbose != 0:  # 打印与记录损失
                writer.add_scalar(tag='train_batch_avg_loss', scalar_value=l.item() / X.size(0),
                                  global_step=epoch * batch_num + i)  # 使用writer记录batch_avg_loss
         # 计算train_accuracy
        _, train_acc = AlexNet_eval(net, train_loader, verbose=verbose)
        # 计算valid_accuracy
        valid_loss, valid_acc = AlexNet_eval(net, valid_loader, verbose=verbose)
        writer.add_scalars(main_tag='epoch_avg_loss', tag_scalar_dict={'train': train_loss, 'valid': valid_loss},
                           global_step=epoch)
        writer.add_scalars(main_tag='epoch_acc', tag_scalar_dict={'train': train_acc, 'valid': valid_acc},
                           global_step=epoch)
```


# 3. 实验结果与分析
`train_loader, valid_loader, test_loader, out_features=load_caltech_101 (32)`
`lr, num_epochs = 0.01, 20
`log_batch = 5`
这一组超参数的训练结果：
![[Pasted image 20230507012106.png]]

![[Pasted image 20230507012128.png]]

`train_loader, valid_loader, test_loader, out_features=load_caltech_101 (64)`
`lr, num_epochs = 0.005, 40`
`log_batch = 5`
这一组超参数的训练结果：
![[Pasted image 20230507025508.png]]

![[Pasted image 20230507025544.png]]

第一组的评价数据集损失小一些，第二组的训练数据集损失小一些。

其中第二组模型在测试集上的评分（第一组产生的模型忘单独保存了被第二组的文件替代了）：
![[Pasted image 20230507220018.png]]

![[Pasted image 20230507220031.png]]